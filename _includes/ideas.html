<div class="news">
  <style>
body {
  font-family: 'Roboto', serif;
}
</style>

    <font  size="+0">
  <h1><strong>About</strong> me</h1>

  <p><strong>Applying state-of-the-art research in production.</strong></p>
  <ul style="list-style: disc;">


 <li> My primary strength lies in my profound and extensive comprehension of the literature in deep learning. Ranging from low level optimization of neural networks computations to theoretical machine learning passing by reinforcement learning, bayesian probabilities or computational neuroscience.
    I can quickly gain an in-depth understanding of any paper from top machine learning conferences (<a href="https://nips.cc/">NIPS</a>/<a href="https://iclr.cc/">ICLR</a>/<a href="https://icml.cc/">ICML</a>) and be able to reproduce it or implement it efficiently 
    in production which I think is a very rare and valuable skill.</li>

 <br>




</ul>


  <h1><strong>What I </strong> believe</h1>

  <p><strong>The advantage of digital intelligence over biological intelligence.</strong></p>
   <ul style="list-style: disc;">


  <li> One significant distinction between digital intelligence and biological intelligence is that biological neural networks weights are physical, 
    the hardware is tied to the software, that is humans and animals are <a href="https://www.youtube.com/watch?v=sghvwkXV3VU&ab_channel=VectorInstitute">mortal computers</a>.
    This distinction underscores the unique advantage of digital intelligence in its ability to run identical weights on diverse physical substrates, 
    providing interesting properties unattainable by biological counterparts </li>

  <br>


  <li> This property is very important since it means that all individual copies of the model can share the same weights allowing them to communicate what they have learned from their 
   individual training data by sharing gradients. Current <a href="https://openai.com/research/techniques-for-training-large-neural-networks">large neural networks 
    leverage this property via parallelism</a> to process vast amount of data and acquire tremendous amount of knowledge
   , that is how something like GPT-4 is able to read "everything that is on internet" something that no human today is able to do.</li>

  <br>


   <li> Illustratively, consider an artificial neural network radiologist who has analyzed tens of thousands of patients, compared to a human counterpart who has encountered only a few thousand.
    The artificial neural network radiologist will attains a significantly higher level of understanding and efficiency. This mirrors the paradigm of AlphaGo excelling as the premier Go player through extensive gameplay, surpassing what any human could achieve in a single lifetime.
  </li>

  <br>


  <li> A common myth is that current artificial neural networks are more statistically inefficient than humans, This misconception often arises from unfair comparisons, such as pitting the learning abilities of a university undergrad against a blank neural network, 
    without accounting for the wealth of prior knowledge humans possess. Recent studies, conducting a <a href="https://openreview.net/pdf?id=W23ZTdsabj">more rigorous comparison</a>, and the demonstrated ability of recent models in few-shot learning refute this myth, 
    showcasing that artificial neural networks are just as statistically efficient as humans.  </li>
  <br>


 





</ul>


<p><strong>About a theory of intelligence and cognition.</strong></p>

<ul style="list-style: disc;">


    
  <li><a href="https://en.wikipedia.org/wiki/Free_energy_principle">The free energy principle </a> is currently the most convincing theory of cognition, it is very powerful in that it doesn't only try to explain what cognition is, it explains the mathematical rules that any agent doted with cognition should follow in any possible world. The brain can be viewed as a particular solution nature found to implement approximate bayesian inference in biological organisms and AI research can be viewed as finding the best ways to implement it into machines which what Solomonoff's <a href="https://raysolomonoff.com/publications/alp-theory-and-applications.pdf">algorithmic probability theory</a> is about. </li>

  <br>


  <li>A lot of complex mental phenomena previously thought to be mysterious make a lot more sense under this paradigm, such as <a href="https://arxiv.org/abs/2009.01791">perception and action</a>, <a href="https://grazianolab.princeton.edu/sites/g/files/toruqf3411/files/graziano/files/jcs_graziano_2016.pdf">consciousness</a>, its <a href="https://arxiv.org/pdf/2302.06403.pdf">ineffability</a>, <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">emotions</a>, and <a href="https://arxiv.org/pdf/2008.07408.pdf">selfhood</a>, or mental disorders such as <a href="https://www.nature.com/articles/nrn2536">schizophrenia</a>, <a href="https://www.quentinhuys.com/pub/HuysEa14-VulnerabilityAddictionDopamine.pdf">addiction</a>, and maybe even <a href="https://www.deepmind.com/blog/dopamine-and-temporal-difference-learning-a-fruitful-relationship-between-neuroscience-and-ai">depression</a>. </li>
  <br>


  <li> These theoretical works among other lead me to think that we have a decent understanding  of the principles underlying cognition and intelligence. The current focus appears to be shifting towards the engineering aspects, specifically addressing the practicalities of constructing these systems.
    We are in a situation similar to understanding the principles of electromagnetism and the nature of a photon but still needing to figure out how to build a laser.
  </li>
  <br>



</ul>


      </table>
    </div>
    
    </font>
</div>
