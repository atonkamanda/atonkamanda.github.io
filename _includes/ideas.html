<div class="news">
  <style>
body {
  font-family: 'Roboto', serif;
}
</style>

    <font  size="+0">
  <h1><strong>About</strong> me</h1>

  <p><strong>Applying state-of-the-art research in production.</strong></p>
  <ul style="list-style: disc;">



  <li>My primary strength lies in translating cutting-edge deep learning research into practical, high-performance solutions. Through my previous experience with state-of-the-art models, I've navigated the challenges of transitioning from research to production. This has given me the ability to quickly grasp complex papers from top machine learning conferences and efficiently implement them in real-world environments.</li>
</ul>

 <br>




</ul>


 <p><strong>Building Neural Networks</strong></p>
   <ul style="list-style: disc;">
  <li>Artificial neural networks are digital, allowing all copies of a model to share weights across diverse physical substrates and communicate learned information via different parts of the data through gradient sharing. This property enables <a href="https://openai.com/research/techniques-for-training-large-neural-networks">large neural networks to leverage parallelism</a>, processing vast amounts of data and acquiring knowledge at a scale unattainable by humans. This is exemplified by Large Language Models' (LLMs) ability to process almost all the content on the internet, or by AlphaGo's mastery of Go through extensive gameplay beyond what any human could achieve in a lifetime. These capabilities make the ability to leverage large amounts of compute and data crucial skills for producing systems with superhuman abilities.</li>
  <br>
  <li><a href="https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/">"The 'it' is in the dataset"</a>;  given sufficient weights and training time, models trained on the same dataset tend to converge to similar representations. This implies that model behavior is primarily determined by the dataset and the loss function, while factors like architecture, hyperparameters, or optimizer are mainly involved in delivering the compute necessary to reach those representations efficiently.</li>
  <br>
  <li>The most effective way to improve a model's performance on a task is by developing an intuition about how the model interacts with the data. There are unique subtleties in the training dynamics that can only be learned through consistent training and evaluation. For example, <a href="https://arxiv.org/abs/2304.03843">LLMs learn reasoning only if the dataset exhibits certain properties</a>, regardless of the architecture, and they <a href="https://arxiv.org/abs/2405.17399">struggle with arithmetic without proper embeddings</a>. Understanding these nuances is crucial for optimizing the learning process as a whole.</li>
  <br>
  <li>A concrete example that exemplifies this principle is the <a href="https://arxiv.org/abs/1812.03982">SlowFast Networks for Video Recognition</a> paper. The authors leverage a property of the data distribution in videos: spatial semantic features (the content of the video) evolve more slowly than temporal dynamic features (e.g., the actions being performed). To build a model that learns the task more efficiently, one needs to consider the properties of both the data and the model jointly, designing architectures that align with the inherent structure of the data.</li> 
  <br>
</ul>

</ul>



 





</ul>


<p><strong>About a theory of intelligence and cognition.</strong></p>

<ul style="list-style: disc;">


    
  <li><a href="https://en.wikipedia.org/wiki/Free_energy_principle">The free energy principle </a> is currently the most convincing theory of cognition, it is very powerful in that it doesn't only try to explain what cognition is, it explains the mathematical rules that any agent doted with cognition should follow. The brain can be viewed as a particular solution nature found to implement approximate bayesian inference in biological organisms and AI research can be viewed as finding the best ways to implement it into machines with computational restraints which what Solomonoff's <a href="https://raysolomonoff.com/publications/alp-theory-and-applications.pdf">algorithmic probability theory</a> is about. </li>

  <br>


  <li>A lot of complex mental phenomena previously thought to be mysterious make a lot more sense under this paradigm, such as <a href="https://arxiv.org/abs/2009.01791">perception and action</a>, <a href="https://grazianolab.princeton.edu/sites/g/files/toruqf3411/files/graziano/files/jcs_graziano_2016.pdf">consciousness</a>, its <a href="https://arxiv.org/pdf/2302.06403.pdf">ineffability</a>, <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">emotions</a>, and <a href="https://arxiv.org/pdf/2008.07408.pdf">selfhood</a>, or mental disorders such as <a href="https://www.nature.com/articles/nrn2536">schizophrenia</a>, <a href="https://www.quentinhuys.com/pub/HuysEa14-VulnerabilityAddictionDopamine.pdf">addiction</a>, and maybe even <a href="https://www.deepmind.com/blog/dopamine-and-temporal-difference-learning-a-fruitful-relationship-between-neuroscience-and-ai">depression</a>. </li>
  <br>





</ul>


      </table>
    </div>
    
    </font>
</div>
